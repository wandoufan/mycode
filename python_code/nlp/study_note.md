
机器学习中相关概念：

* 机器学习是一种重在寻找数据中的模式并使用这些模式来做出预测的研究和算法的门类。
机器通过分析大量数据来进行学习。比如说，不需要通过编程来识别猫或人脸，它们可以通过使用图片来进行训练，从而归纳和识别特定的目标。

训练集，验证集，测试集的作用：
在训练有监督的机器学习模型的时候，会将一个大的数据集划分为训练集、验证集和测试集，划分比例一般设置为8:1:1，划分方式为均匀随机抽样。
对原始数据进行三个集合的划分，是为了能够选出效果（可以理解为准确率）最好的、泛化能力最佳的模型。
将数据分为三个部分也是为了防止产出的模型过拟合，如果用所有的数据去训练模型，得到的结果会过拟合与原始数据，而对新数据进行训练时效果可能更差。
需要注意的是，通常都会给定训练集和测试集，而不会给验证集。这时候验证集一般都是从训练集中均匀随机抽样一部分样本作为验证集。

* 原始数据集：已经经过正确处理的大的数据集合

* 训练集(Training set)：算法从训练集中挖掘出规则，组合成分类器，并产出训练模型
即通过训练集来确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数。

* 验证集(Validation set)：用来做模型选择，即模型的最终优化和确定
验证集并不参与学习参数的确定，验证集只是为了选择超参数

* 测试集(Test set)：测试集只使用一次，即在训练完成后评价最终的模型时使用。它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型效果的评价
其中训练集和验证集均是同一对象的数据，但是测试，我们就需要用跨对象的数据来验证模型的稳定性。
值得注意的是，千万不能在训练过程中使用测试集，而后再用相同的测试集去测试模型。这样做其实是一个cheat，会造成模型过拟合。

* 超参数不能或者难以通过机器学习算法学习得出，一般由专家经过经验或者实验选定，如广义线性回归中的多项式次数，控制权值衰减的λ等

* 训练轮数：
???

* 学习方法：使用样例（或称样本，训练集）来合成计算机程序的过程称为学习方法。

* 监督学习：学习过程中使用的样例是由输入/输出对给出时，称为监督学习。最典型的监督学习例子就是文本分类问题，训练集是一些已经明确分好了类别文档组成，文档就是输入，对应的类别就是输出。

* 非监督学习：学习过程中使用的样例不包含输入/输出对，学习的任务是理解数据产生的过程。典型的非监督学习例子是聚类，类别的数量，名称，事先全都没有确定，由计算机自己观察样例来总结得出。

* 假设：计算机对训练集背后的真实模型（真实的分类规则）的猜测称为假设。

* 泛化性：一个假设能够正确分类训练集之外的新的未知数据的能力称为该假设的泛化性。

* 一致假设：一个假设能够对所有训练数据正确分类，则称这个假设是一致的。

* 过拟合：为了使匹配结果更精准而把规则设置的过于严格称为过拟合，过于严格的规则匹配样本数据会非常精准，但对于新数据匹配结果可能反而更差。

* 正样本和负样本：对某个类别来说，属于这个类别的样本文档称为正样本；不属于这个类别的文档称为负样本。

* 超平面：n维空间中的线性函数唯一确定了一个超平面。一些较直观的例子，在二维空间中，一条直线就是一个超平面；在三维空间中，一个平面就是一个超平面。

* 线性可分和不可分：如果存在一个超平面能够正确分类训练数据，并且这个程序保证收敛，这种情况称为线形可分。如果这样的超平面不存在，则称数据是线性不可分的。

-------------------------------------------------------------------------

对机器学习模型进行性能度量中的相关概念：
1.在回归任务中：
最常用的性能度量是均方误差，即多次样本的结果和标准结果进行差值比对
差值越小代表越接近于标准结果
2.在分类任务中(包括二分类任务和多分类任务)：
错误率：分类错误的样本数占样本总数的比例
精度：分类正确的样本数占样本总数的比例
即错误率+精度=1
以判断是否是恶心肿瘤为例
查准率(precision):判断是恶性肿瘤数/(判断是恶性肿瘤数+判断是良性肿瘤数)，描述准确性
查全率(recall)/召回率：判断是恶性肿瘤数/样本中所有恶性肿瘤个数，描述全面性
查准率和查全率是一组矛盾的参数；想要增加准确率，就需要把最具把握的样本才判定
为恶性肿瘤；想要增加全面率，就需要把所有具有可能性的样本都判定为恶性肿瘤
F1参数用来平衡查准率和查全率，在不同的应用场景下来提供查准率或查全率
https://blog.csdn.net/ybdesire/article/details/53613628
https://blog.csdn.net/liweibin1994/article/details/76944056
https://www.cnblogs.com/zle1992/p/6689136.html

-------------------------------------------------------------------------

中文与英文的区别（为什么首先要做分词）：
对于我们每天打交道的中文来说，并没有类似英文空格的边界标志。
而理解句子所包含的词语，则是理解汉语语句的第一步。汉语自动分词的任务
，通俗地说，就是要由机器在文本中的词与词之间自动加上空格。
目前常用的分词方法有两类：
机械式分词法：以分词词典为依据，对文本中的字符串和词表中的词逐一匹配进行分词。
理解式分词法：利用汉语的语法知识，语义知识和心理学知识，需要建立分词数据库，
知识库和推理库，属于理想的方法，但目前还不成熟。

--------------------------------------------------------------------------

文本分类相关：
https://blog.csdn.net/banrixianxin/article/details/25928967
https://www.douban.com/note/627665166/?type=like

* 自动文本分类(Automatic Text Categorization)简称文本分类，是指计算机将一篇文章归于预先给定的某一类或某几类的过程。
文本分类会按照预先定义的主题类别，为文档合集中的每个文档确定一个类别。
文本分类问题与其它分类问题没有本质上的区别，其方法可以归结为根据待分类数据的某些特征来进行匹配，当然完全的匹配是不太可能的，因此必须（根据某种评价标准）选择最优的匹配结果，从而完成分类。

* 需要注意的两个点：
1.用于分类的类别体系是需要提前建立的，而且一旦确定就不能轻易改变，
如果要改变就相当于重建一个分类系统
2.一个文本没有严格规定只能被分配给一个类别，分在不同类别的置信度可能不一样

* 文本分类的应用场景：
对文本集按照主题进行分类，判断文章写作风格，判断作者态度，判断是否是机器人
写的文章，判断是否是垃圾邮件，等等.....

* 文本分类和网页分类的区别：
文本分类类似于网页分类，但网页分类可以借助文件的编码格式，文章作者，发布日期
，网页链接，网页结构等信息进行分类判断，而文本分类只能根据文章的文字内容进行分类

* 文本分类的几种方法：
1.词法匹配，最初版的方法通过判断文本中是否出现指定关键词来判断文本的类别，但实际的效果不好。
2.知识工程，改进版的方法通过人工来提前对文本进行分析，为文本建立起推理匹配规则，
但这种方法的效果严重依赖于匹配规则的质量，而且一套规则只能针对一类文本，成本较高且不具有灵活性。
3.目前常用是统计学习方法(机器学习方法)，需要提前准备一批由人工进行了精准分类
的文档作为学习材料(即训练集/样本/样本数据)，计算机自动从文档中挖掘出来有效的分类规则
(过程称为训练/training)，总结出来的规则合集称为分类器，训练完成后就可以对新文档使用分类器进行分类处理。
常用的算法有支持向量机SVM,朴素贝叶斯,KNN等
4.深度学习方法是对统计学习方法的改进，统计机器学习方法首先需要进行特征工程工作，该工作需要深入理解业务需求，并且非常耗时耗力。
而深度学习方法可以自动的提取特征，使人们将注意力集中在数据和模型上。
常用的算法有CNN模型,fastText模型等。

* 一般把学习模型在实际使用中遇到的数据称为测试数据；为了加以区分，模型评估
与选择中用于评估测试的数据集称为验证集

* 统计学习方法的思想：
一般认为文档的内容与其中所包含的词有着必然的联系，同一类文档之间总存在多个共同的词，而不同类的文档所包含的词之间差异很大；另外词出现的次数对分类也很重要。
空间向量模型(VSM)把样本数据转化为向量表示，通过记录关键词和词的权重来实现对文本的解析，实现简单，准确度也较高。词的权重不一定就是词频，更常用的是TF-IDF参数。
文本作为信息的载体包含的信息包括：词元素本身的信息，元素之间组成顺序带来的信息，以及上下文之间的信息。而VSM模型只用到了词本身信息，忽略了其他方面的信息。
潜在语义索引(LSI)模型相比VSM模型可以保留更多的语义信息，但目前还不成熟。

* 统计学习方法的实现过程：
1.把文本转化成向量表示
存在的问题是包含所有中文词语的词典向量太大，用来存储一篇只有几千词的文本会浪费空间。
首先会进行一个'去停止词/停用词'的预处理过程，即根据停用词词典，去掉文本带有一些通用的,无法用来判断文本类别的词语，如'我们'，'事情'，'里面'等词。
注意：'去停用词'的场景比较模糊，不经常会进行该步骤
另外会通过'特征提取'/'降维'/'TSR'(Term Space Reduction)特征空间的压缩过程来选取出最具代表性的词语，来进一步减少词典向量。
特征提取有两类方法：
特征选择：从原有特征中选出少量更具有代表性的特征，特征的数量减少但类型并未变化。
特征抽取：从原有特征中重构出新的特征，如LSI模型转化为矩阵，文档生成模型转化为概率分布参数。新特征更具代表性，且耗费资源更少。
2.构造分类器
对同一个文本数据，同一个向量模型，关注不同的特性不同的指标就会得到不同的结论。
常见的分类算法有决策树，Rocchio，朴素贝叶斯(Naive Bayes)，神经网络，支持向量机，线性最小平方拟合，kNN，遗传算法，最大熵，Generalized Instance Set等等，但这些算法都有各种缺陷。
目前性能最好的算法是支持向量机(Support Vector Machine)/SVM算法，SVM分类器的优点在于通用性较好，且分类精度高,分类速度快,分类速度与训练样本个数无关，在查准和查全率方面都优于kNN及朴素贝叶斯方法。






