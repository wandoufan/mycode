# ANN(artificial neural networks)人工神经网络  
神经网络可以看做是一个能够拟合任意函数的黑盒子，只要有足够的训练数据，对于给定的X输入，就能得到期望的Y输出

## 参考资料  
> https://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&mid=2652248024&idx=1&sn=173e696a16eccb618a8725482c381fbf&chksm=80cc8e7db7bb076bfd6916135b30f0ed54e240dc971422247b98bf1e6b6d0561ef3563eb0c8e&scene=21#wechat_redirect

## 相关概念  

### 函数饱和性：  
* 当函数输入值很大时，随着输入值的继续增加，输出结果几乎没有变化，会导致梯度消失，这称为函数具有饱和性  
* 饱和按照输入值x的取值范围分为左饱和和右饱和，但函数同时满足左饱和和右饱和时称为饱和函数  
* 随着输入值x的增加输出值y恒定在一个值上称为硬饱和，如阶跃函数  
* 随着输入值x的增加输出值y缓慢变化称为软饱和， 如sigmoid函数  

### 梯度消失：  
* 在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了，这种现象叫做消失的梯度问题  

## 神经元  

### 一个神经元  
公式样例： z = k1*x + k2*y +k3  
函数图像： 一根直线把平面分成两半  
应用场景： 在简单线性场景下当做分类器用于分类  

## 激活函数  

### 激活函数的作用  


### 激活函数的特性  
* 激活函数应该是一条顺滑的曲线，即处处可导  
* 激活函数应该是非线性的，不能是一条直线，即导数不是常数  
* 激活函数的输入值可以在[−∞,+∞]之间，但应该是有限的输出范围  
* 激活函数应该具有单调性，只能单调增长或单调减少，即导数符号只能正或负  
* 激活函数应该具有非饱和性，否则会造成系统收敛迭代慢  

### 常用激活函数样例  
* sigmoid函数，优点是单调连续，输出范围有限，求导容易；缺点是具有软饱和性，容易产生梯度消失，导致训练出现问题，另外输出范围也不是以0为中心的。  
```
def sigmoid(x):
    """
    Sigmoid函数是单增长的S型函数，常被用作神经网络的阈值函数，可以将变量x(x范围在负无穷到正无穷之间)映射到0,1之间
    其中math.exp(-x)代表自然常数e的-x次方， 函数图像是第一二象限间的S型的单调增长函数，其中x = 0时y = 1/2
    """
    return 1.0 / (1 + math.exp(-x))
```
* tanh函数，优点是比sigmoid函数收敛速度更快，输出范围以0为中心；缺点是同样存在饱和性产生的梯度消失问题。  
```
def tanh(x):
    """
    tanh函数即双曲正切函数，可以将变量x(x范围在负无穷到正无穷之间)映射到-1,1之间
    函数图像是第一三象限间的S型单调增长函数，其中x = 0 时y = 0
    """
    return (math.exp(x) - math.exp(-x))/(math.exp(x) + math.exp(-x))
```
* ReLU函数，优点是能够更快速的收敛，有效缓解了梯度消失的问题，在无监督的训练中也有较好的效果；缺点是训练中可能会出现神经元死亡，权重无法更新的情况。  
```
def relu(x):
    """
    relu函数即线性整流函数，又称线性修正单元，具有线性、非饱和的特性
    """
    if x <= 0:
        return 0
    else:
        return x
```

## BP(back propagation)误差反向传播算法  

### 神经网络的结构  
神经网络是一种分层结构，一般由输入层、隐藏层、输出层组成，即神经网络至少有3层  
当隐藏层大于1，总层数大于3时就可以成为深度学习  
* 输入层：接收原始数据，然后传送到隐藏层  
* 隐藏层：对数据进行特征提取，把前一层的输出变成后一层的输入，本质上相当于对数据进行空间坐标变换  
* 输出层：神经网络的决策输出  

### 反向传播  

### 正向传播  

### 学习率