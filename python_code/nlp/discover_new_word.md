# 新词发现

## 参考资料
> https://blog.csdn.net/smartcat2010/article/details/77883735
> https://github.com/Moonshile/ChineseWordSegmentation
> https://github.com/yanghanxy/New-Word-Detection

## 功能简介
新词发现实际是一种不依赖任何现有词库的分词算法，可以通过分析文本片段的各种属性来判断文本片段成词的可能性。  
首先从文本中找出所有可能成词的文本片段，然后和现有的词典进行比对，词典中没有收录的即为发现的新词  

## 应用场景
在nlp中，对中文文本处理的第一步是分词，分词的重要基础就是词典  
现有的词典存在的一个重大缺陷就是词普遍老旧，未能及时收录新出现的词语，导致了对一些新词进行切分时效果不佳  
新词的出现很大程度上影响自动分词工具的准确性，研究显示,60% 的分词错误是由新词导致的  
中文环境下每天都有大量的新词语出现，例如网络新词、机构名、品牌名、人名、缩略词等等，这些新词的产生机制完全无规律可寻  
因此一套自动发现并收录新词的系统就对分词工作非常重要  

## 常用方法
1. 基于规则的方法利用构词原理,结合语义、词性等信息构造模板,通过匹配来发现新词  
基于规则的方法准确率高，但需要根据具体的领域来制定不同的规则，维护规则较困难，可扩展性也比较差  
2. 基于统计的方法通过对语料中的词频，左右熵，聚合度等文本片段的属性信息进行统计来发现新词  
基于统计的方法灵活度比较高，目前实际采用这种方法进行新词发现  
根据统计方法的特性，一般文本规模越大，发现的候选词准确率越高，更适合于从大规模语料中获取新词  

## 处理流程
1. 文本语料预处理  
1.1 语料清洗  
由于文本语料的来源广泛，文本语料的格式规范也有很大的不确定性，因此对于用词发现的文本语料要预先进行数据清洗。  
首先去掉文本中的换行符等各种空白符，将文本中每一句话都拼接起来。  
然后将文本中的标点符号替换为空格，关于标点符号可以根据新词发现任务的需求来保留相应的符号。  
例如，在进行军事武器词的新词发现时，目标新词中经常会出现‘歼-20’中的‘-’，因此‘-’就需要保留下来。  
1.2 语料切割  
根据目标新词的最大词长来对语料进行切割，最大词长与具体的新词发现任务有关。  
根据统计方法的特性，词长越短，统计结果的准确度越高。  
一般我们认为中文中至少两个字才能成词，以最大词长等于4为例，我们对文本从头到尾分别切割出所有可能的长度为1、2、3、4的文本片段作为候选词。  
备注：长度为1的单字本身不能成词，但是在计算长词的聚合度时会需要用到单字词频信息  
2. 成词标准  
对于每一个候选词，可能是一个完整的词，也可能是一个不成词的文本片段。  
通过判断候选词的信息熵可以计算出候选词成词的概率。  
2.1 聚合度  
聚合度又称为内部凝固度或凝合程度，用来表示文本片段内部的字之间的结合紧密程度。  
聚合度 = 文本片段词频 / 文本片段子部分词频乘积。  
聚合度高表示文本片段内部的字之间经常是固定搭配，而不是由于随机排列被组合在了一起，例如：'蝙蝠'、'忐忑'、'乒乓'等。  
一般我们认为词语内部的字都会经常性组合在一起，即聚合度越高的候选词成词的概率越大。  
以候选词‘abc’为例，其聚合度的计算公式为：  
Aggregation =min⁡{P(abc)/P(ab)P(c) ,(P(abc))/(P(a)P(bc))}  
2.2 左右熵  
左右熵表示文本片段外部左/右搭配字的丰富程度,即自由运用程度。  
一般我们认为一个文本片段能够成词则它的左右两边就能搭配很多的文字，即有丰富的左邻字集合和右邻字集合。   
例如，语料为'吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮'，则其中候选词’葡萄’的左邻字集合为{吃,吐,吃,吐}，右邻字集合为{不,皮,倒,皮}，’葡萄’的左右熵计算公式为：  
left_entropy = -(1/2)*ln(1/2)-(1/2)*ln(1/2) ≈ 0.693  
right_entropy = -(1/2)*ln(1/2)-(1/4)*ln(1/4)-(1/4)*ln(1/4) ≈ 1.04  
3. 候选词进一步过滤筛选  
3.1 阈值及综合分数  
针对候选词可以分别设定聚合度、左右熵、出现词频的最小值，将不符合阈值的候选词都过滤掉，可以减少文本中由于随机性产生的不成词的文本片段。  
结合候选词的各项属性值计算出一个综合分数，计算公式为：  
Score = (left_entropy+right_entropy)*aggregation*frequence  
将候选词按照综合分数由高到低排序，去掉低分段的候选词，只保留高分段的候选词。  
3.2 左右停用字  
在从语料切割出文本片段中经常会存在一些无意义的助词、语气词、量词等，如’的’,’了’,’是’等。  
可以从一个大规模的语料中统计出文本片段左右两侧最常出现的字，经过人工筛选后作为左右停用字表。  
如果候选词的左右两侧字出现在停用字表中，则认为该候选词是一个无意义的不成词的文本片段，可以将其过滤掉。  
3.3 特殊的badcase  
在中文环境下，存在一类文本片段，它的内部聚合度很高，左右两侧也有丰富的邻字集合，出现频次也很高，但它并不是一个完整的词，而是一类特殊的badcase。  
如’斯坦’本身不是一个词，但经常被包含在’巴基斯坦’，’斯坦福’,’哈萨克斯坦’中，各项指标分数都很高，会通过层层过滤出现在最终结果中。  
因此对于这一类特殊badcase需要专门再进一步过滤，目前暂时没有好的解决办法，可以通过人工筛选后加入停用词表来过滤。  
3.4 利用现有词表过滤  
经过以上过滤后的候选词，就认为是可以成词的文本片段。将候选词和现有的词表进行比对，没有出现在现有词表中的候选词就是我们要找的目标新词。  
3.5 人工标注  
由于中文语言的复杂性，经过算法层层过滤得到的结果中还可能混杂有不成词的文本片段。  
在工业级的应用中还是需要经过人工标注进行最后的确认。  

## 算法优化实现细节
1. 通过只计算出n元文本片段的左右熵来近似替代n元以上文本片段的左右熵，其中n的取值范围为(1,最大词长)  
这是一种通过降低准确度来提高时间效率的做法  

一篇提高左右熵计算速度的论文，待研究：  
> https://wenku.baidu.com/view/2738b812cc7931b765ce1541.html

2. 将空格也计入左右邻字集合中，并对没有左右邻字的情况补充空格  

3. 计算熵时分别计算空格和非空格字符  
假设某个候选词左邻字集合为{'a', 'b', ' ', ' '}，即集合中含有2个普通字符和2个空格  
两种计算方法：
a. 空格按照概率公式单独计算
entropy = -(1/4)*ln(1/4) -(1/4)*ln(1/4) + 2/4  
b. 空格合并入熵计算公式 
entropy = -(1/4)*ln(1/4) -(1/4)*ln(1/4) - 2*(1/4)*ln(1/4)   
注意： b方法中每个空格都单独算作一种字符，而不是所有空格加起来算作一种字符  

4. 通过判断文本片段左邻字中数量词来过滤半个词的badcase  