# 关于新词发现的相关内容：
# https://blog.csdn.net/smartcat2010/article/details/77883735
# 算法python3实现参考：
# https://github.com/Moonshile/ChineseWordSegmentation
# https://github.com/yanghanxy/New-Word-Detection

# -------------------------------------------------------------------------------------------------

# 新词发现的目的/作用：
# 在中文nlp中，对文本处理的第一步是分词，分词的的重要基础就是词典
# 现有的词典存在的一个重大缺陷就是词普遍老旧，未能及时收录新出现的词语，导致了对一些新词进行切分时效果不佳
# 中文环境下每天都有大量的新词语出现，例如网络新词、机构名、品牌名、人名、缩略词等等，这些新词的产生机制完全无规律可寻
# 因此一套自动发现并收录新词的系统就对分词工作非常重要

# -------------------------------------------------------------------------------------------------

# 新词发现的基础理论：
# 凝合程度/聚合程度:表示文本片段内部的字结合紧密程度
# 通过凝合程度来找到词的边界
# 例如,电影院的凝合程度计算为：
# aggregation('电影院') = min[p('电影院')/(p('电')*p('影院')),p('电影院')/(p('电影')*p('院'))]
# 凝合程度高表示文本片段内部的字之间经常是固定搭配,例如：'蝙蝠'、'彷徨'、'玫瑰'、'忐忑'、'乒乓'等等
# 凝合程度越高说明文本片段能成词的概率越大

# 左熵/右熵:表示文本片段外部左/右搭配字的丰富程度,即自由运用程度
# 通过左右熵来判断是否是一个词
# 由于词本身的特性，可能会存在左熵很大右熵很小或右熵很大左熵很小的情况
# 例如，'交响'，'后遗'，'鹅卵'等词
# 自由运用程度：定义为文本片段的左熵和右熵中较小的值
# 文本片段能够算作词,则片段应该有丰富的左邻字集合和右邻字集合,即自由运用程度高
# 例如,str='吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮','葡萄'左邻字为{吃,吐,吃,吐},右邻字为{不,皮,倒,皮} 
# '葡萄'的左右熵为计算：
# left = -(1/2)*ln(1/2)-(1/2)*ln(1/2) ≈ 0.693
# right = -(1/2)*ln(1/2)- 1/4)*ln(1/4)-(1/4)*ln(1/4) ≈ 1.04

# 在实际判断中，文本片段的凝合程度和自由程度都要考虑；
# 只看凝固程度的话，程序会找出'巧克'、'俄罗'、'颜六色'、'柴可夫'等实际上是'半个词'的片段；
# 只看自由程度的话，程序则会把'吃了一顿'、'看了一遍'、'睡了一晚'、'去了一趟'中的'了一'提取出来;
# 另外会存在一些特殊词，如'利亚'、'斯坦'等凝合度和自由度都很高，但实际并不能成词

# ------------------------------------------------------------------------------------------------

# 调研了网上一些常用的对标点符号等位置信息处理方法：
# 一部分是直接将标点符号删除，然后将字符串拼接起来，这样处理的效果并不好，
# 不仅会丢失标点符号的位置信息，还会在字符串拼接之后给文本片段带来了实际并不存在的左右邻字
# 还有一部分是将标点符号替换为换行符或对标点符号的位置直接进行切分，
# 我在代码中是对语料按行读取，按行处理，如果要把每行语料按照标点符号再次切分就会在双层循环中再增加一层循环，
# 担心过多的循环会影响效率，因此在代码中将标点符号替换为空格
# 另外，针对每一行开头或结尾的没有左邻字或右邻字的文本片段，在网上还没有发现有专门对此进行特别处理的算法

# -------------------------------------------------------------------------------------------------

# 修改/创新：

# 1. 通过只计算出n元文本片段的左右熵来近似替代n元以上文本片段的左右熵，其中n的取值范围为1到k，k为限定的最大词长。

# 对文本进行切分处理后产生所有可能的词长在k以内的文本片段，在这些文本片段中我们把所有n元及以上的文本片段的左/右邻字集合
# 都视作等同于有相同开头/结尾的n元文本片段的左/右邻字集合；同样的，所有n元及以上的文本片段的左/右熵也都等同于有相同
# 开头/结尾的n元文本片段的左/右熵。例如，只统计出'周'的左邻字集合，然后计算出'周'的左熵，'周杰伦'的左熵就可以等同于'周'的左熵。

# 在传统的计算熵过程中需要统计出每个可能的文本片段的左/右邻字集合，进而计算出每个文本片段的左/右熵值。
# 在文本语料规模比较大的情况下切分处理后会产生巨量的文本片段，整个计算过程要消耗非常多的时间。
# 例如，对于一个简单文本'周杰伦的歌'，在限定最大词长为4的情况下传统方法需要统计出['周', '周杰', '周杰伦', 
# '周杰伦的','杰', '杰伦', '杰伦的', '杰伦的歌', '伦', '伦的', '伦的歌', '的', '的歌', '歌']共14个文本片段的左/右邻字集合，
# 并分别计算左/右熵。通过改进后就可以只计算出n元片段的左/右熵，大大减少计算熵的工作量，提高代码效率。

# 但实际上这样操作也会造成一定误差，在同一份语料中，'周杰伦'的左邻字集合一定'周'的左邻字集合，但'周'的左邻字集合不一定是'周杰伦'的
# 左邻字集合。例如，在一份测试语料中'周'有左邻字'上'来组成'上周'，但'周杰伦'并没有存在左邻字'上'，这样操作会造成'周杰伦'的左熵偏高。

# 这个问题在只统计一元文本片段时误差较为明显，当只统计二元或二元以上的文本片段时误差就会大大降低。在同一份语料中只统计n元文本片段时，
# 当n取值越大，误差就越小。一般我们认为两个字就可以成词，因此在实际操作中，当n取值3或以上时还会存在另一个问题：例如，在同一份语料中，
# 我们只统计三元文本片段的左/右邻字集合，但我们不能直接把'周杰伦'的左邻字集合等同于'周杰'的左邻字集合，因为语料中可能还存在'周杰杰'
# 等其他以'周杰'开头的三元文本片段。

# 解决这个问题有两个方法：第一种方法是将所有以'周杰'开头的三元文本片段的左邻字集合合并后作为'周杰'的左邻字集合；
# 第二种方法是同时统计所有二元文本片段和所有三元文本片段的左/右邻字集合。

# 经过多次测试发现，在限定最大词长为4的情况下只统计二元文本片段的左/右邻字集合可以较好的兼顾性能和效果，
# 不仅可以大幅降低程序的运行时间，同时也能保证发现新词结果的准确率。


# 2.左/右邻字为标点符号和不存在的情况都视作有左/右邻字，并合并统计计数。
# 在对文本进行新词发现时会提前对文本中的各种标点符号进行预处理，传统方法直接将标点符号删除的操作会丢失文本片段的位置信息。
# 一般来说，位于标点符号旁边的文本片段的成词概率往往会更高，我们可以把标点符号都替换为空格，然后把空格和空格的出现次数计入
# 文本片段的左/右邻字集合中，这样就可以提高文本片段的左/右熵。同样的，位于文本开头和结尾的文本片段的成词概率也会比较高，
# 但是位于文本开头的文本片段没有左邻字，位于文本结尾的文本片段也没有右邻字。如果直接认为这些文本片段不存在左/右邻字就会
# 降低它的左/右熵，因此当文本片段的左/右邻字不存在时，把左/右邻字也等同为一个空格并计入文本片段的左/右邻字集合中，
# 这样也可以提高文本片段的左/右熵。


# 3.对文本片段进行多次筛选，并综合考虑文本片段的词频，左/右熵，聚合度等信息。
# 首先，过滤掉字长为1的文本片段和中间有空白符、纯英文等不符合要求的文本片段；其次，按照预设的最小词频，最小熵，最小聚合度
# 对属性值过低的文本片段进行过滤；最后，利用现有词表对发现的新词中重复的新词进行过滤。
# 文本片段的最终分数设定等于 (左熵+右熵)*聚合度*词频，对新词结果按分数进行排序，分数越高的文本片段成词的概率越高。

# --------------------------------------------------------------------------------------------------

# 优化后的运行情况：
# 在只统计二元文本片段的左右邻字集合的情况下
# 消耗时间情况：
# 对一份17M的语料文件测试，平均运行时间由原来的145s降低到103s
# 对一份153M的语料文件测试，平均运行时间由原来的1087s降低到了777s
# 将代码经过cython进一步优化后：
# 对一份17M的语料文件测试，平均运行时间由原来的145s降低到60s
# 对一份153M的语料文件测试，平均运行时间由原来的1087s降低到了420s
# 发现新词结果情况：
# 发现的新词数量比之前提高了大约一倍，整体错误率也有所下降


