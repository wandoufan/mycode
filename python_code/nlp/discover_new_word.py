# 关于新词发现的相关内容：
# https://blog.csdn.net/smartcat2010/article/details/77883735
# 算法python3实现参考：
# https://github.com/Moonshile/ChineseWordSegmentation
# https://github.com/yanghanxy/New-Word-Detection

# -------------------------------------------------------------------------------------------------

# 新词发现的简介：
# 新词发现实际是一种不依赖任何现有词库的分词算法，可以通过分析文本片段的各种属性来判断文本片段成词的可能性
# 首先从文本中找出所有可能成词的文本片段，然后和现有的词典进行比对，词典中没有收录的即为发现的新词
# 备注：由于现有词典的质量不佳，目前实际通过调用LTP分词来对发现的文本片段进行过滤
# 新词发现的研究方法大体可分为两种:基于规则的方法,基于统计的方法.
# 1.基于规则的方法利用构词原理,结合语义、词性等信息构造模板,通过匹配来发现新词.
# 基于规则的方法准确率高，但需要根据具体的领域来制定不同的规则，维护规则较困难，可扩展性也比较差
# 2.基于统计的方法通过对语料中的词频，左右熵，聚合度等文本片段的属性信息进行统计来发现新词.
# 基于统计的方法灵活度比较高，目前实际采用这种方法进行新词发现.
# 根据统计方法的特性，一般文本规模越大，发现的候选词准确率越高，更适合于从大规模语料中获取新词

# -------------------------------------------------------------------------------------------------

# 新词发现的目的/作用：
# 在nlp中，对中文文本处理的第一步是分词，分词的重要基础就是词典
# 现有的词典存在的一个重大缺陷就是词普遍老旧，未能及时收录新出现的词语，导致了对一些新词进行切分时效果不佳
# 新词的出现很大程度上影响自动分词工具的准确性，研究显示,60% 的分词错误是由新词导致的
# 中文环境下每天都有大量的新词语出现，例如网络新词、机构名、品牌名、人名、缩略词等等，这些新词的产生机制完全无规律可寻
# 因此一套自动发现并收录新词的系统就对分词工作非常重要

# -------------------------------------------------------------------------------------------------

# 新词发现的基础理论：
# 1.凝合程度/聚合程度:表示文本片段内部的字结合紧密程度
# 通过凝合程度来找到词的边界
# 例如,电影院的凝合程度计算为：
# aggregation('电影院') = min[p('电影院')/(p('电')*p('影院')),p('电影院')/(p('电影')*p('院'))]
# 凝合程度高表示文本片段内部的字之间经常是固定搭配,例如：'蝙蝠'、'彷徨'、'玫瑰'、'忐忑'、'乒乓'等等
# 凝合程度越高说明文本片段能成词的概率越大

# 2.左熵/右熵:表示文本片段外部左/右搭配字的丰富程度,即自由运用程度
# 通过左右熵来判断是否是一个词
# 由于词本身的特性，可能会存在左熵很大右熵很小或右熵很大左熵很小的情况
# 例如，'交响'，'后遗'，'鹅卵'等词
# 自由运用程度：定义为文本片段的左熵和右熵中较小的值
# 文本片段能够算作词,则片段应该有丰富的左邻字集合和右邻字集合,即自由运用程度高
# 例如,str='吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮','葡萄'左邻字为{吃,吐,吃,吐},右邻字为{不,皮,倒,皮} 
# '葡萄'的左右熵为计算：
# left = -(1/2)*ln(1/2)-(1/2)*ln(1/2) ≈ 0.693
# right = -(1/2)*ln(1/2)- 1/4)*ln(1/4)-(1/4)*ln(1/4) ≈ 1.04

# 在实际判断中，文本片段的凝合程度和自由程度都要考虑；
# 只看凝固程度的话，程序会找出'巧克'、'俄罗'、'颜六色'、'柴可夫'等实际上是'半个词'的片段；
# 只看自由程度的话，程序则会把'吃了一顿'、'看了一遍'、'睡了一晚'、'去了一趟'中的'了一'提取出来;
# 另外会存在一些特殊词，如'利亚'、'斯坦'等凝合度和自由度都很高，但实际并不能成词

# ------------------------------------------------------------------------------------------------

# 调研了网上一些常用的对标点符号等位置信息处理方法：
# 一部分是直接将标点符号删除，然后将字符串拼接起来，这样处理的效果并不好，
# 不仅会丢失标点符号的位置信息，还会在字符串拼接之后给文本片段带来了实际并不存在的左右邻字
# 还有一部分是将标点符号替换为换行符或对标点符号的位置直接进行切分，
# 我在代码中是对语料按行读取，按行处理，如果要把每行语料按照标点符号再次切分就会在双层循环中再增加一层循环，
# 担心过多的循环会影响效率，因此在代码中将标点符号替换为空格
# 另外，针对每一行开头或结尾的没有左邻字或右邻字的文本片段，在网上还没有发现有专门对此进行特别处理的算法

# -------------------------------------------------------------------------------------------------

# 修改/创新：

# 1. 通过只计算出n元文本片段的左右熵来近似替代n元以上文本片段的左右熵，其中n的取值范围为1到k，k为限定的最大词长。

# 对文本进行切分处理后产生所有可能的词长在k以内的文本片段，在这些文本片段中我们把所有n元及以上的文本片段的左/右邻字集合
# 都视作等同于有相同开头/结尾的n元文本片段的左/右邻字集合；同样的，所有n元及以上的文本片段的左/右熵也都等同于有相同
# 开头/结尾的n元文本片段的左/右熵。例如，只统计出'周'的左邻字集合，然后计算出'周'的左熵，'周杰伦'的左熵就可以等同于'周'的左熵。

# 在传统的计算熵过程中需要统计出每个可能的文本片段的左/右邻字集合，进而计算出每个文本片段的左/右熵值。
# 在文本语料规模比较大的情况下切分处理后会产生巨量的文本片段，整个计算过程要消耗非常多的时间。
# 例如，对于一个简单文本'周杰伦的歌'，在限定最大词长为4的情况下传统方法需要统计出['周', '周杰', '周杰伦', 
# '周杰伦的','杰', '杰伦', '杰伦的', '杰伦的歌', '伦', '伦的', '伦的歌', '的', '的歌', '歌']共14个文本片段的左/右邻字集合，
# 并分别计算左/右熵。通过改进后就可以只计算出n元片段的左/右熵，大大减少计算熵的工作量，提高代码效率。

# 但实际上这样操作也会造成一定误差，在同一份语料中，'周杰伦'的左邻字集合一定'周'的左邻字集合，但'周'的左邻字集合不一定是'周杰伦'的
# 左邻字集合。例如，在一份测试语料中'周'有左邻字'上'来组成'上周'，但'周杰伦'并没有存在左邻字'上'，这样操作会造成'周杰伦'的左熵偏高。

# 这个问题在只统计一元文本片段时误差较为明显，当只统计二元或二元以上的文本片段时误差就会大大降低。在同一份语料中只统计n元文本片段时，
# 当n取值越大，误差就越小。一般我们认为两个字就可以成词，因此在实际操作中，当n取值3或以上时还会存在另一个问题：例如，在同一份语料中，
# 我们只统计三元文本片段的左/右邻字集合，但我们不能直接把'周杰伦'的左邻字集合等同于'周杰'的左邻字集合，因为语料中可能还存在'周杰杰'
# 等其他以'周杰'开头的三元文本片段。

# 解决这个问题有两个方法：第一种方法是将所有以'周杰'开头的三元文本片段的左邻字集合合并后作为'周杰'的左邻字集合；
# 第二种方法是同时统计所有二元文本片段和所有三元文本片段的左/右邻字集合。

# 经过多次测试发现，在限定最大词长为4的情况下只统计二元文本片段的左/右邻字集合可以较好的兼顾性能和效果，
# 不仅可以大幅降低程序的运行时间，同时也能保证发现新词结果的准确率。


# 2.左/右邻字为标点符号和不存在的情况都视作有左/右邻字，并合并统计计数。
# 在对文本进行新词发现时会提前对文本中的各种标点符号进行预处理，传统方法直接将标点符号删除的操作会丢失文本片段的位置信息。
# 一般来说，位于标点符号旁边的文本片段的成词概率往往会更高，我们可以把标点符号都替换为空格，然后把左右两侧的空格数量也进行统计，
# 这样就可以提高修正位于标点符号旁边的文本片段的左/右熵。同样的，位于文本开头和结尾的文本片段的成词概率也会比较高，
# 但是位于文本开头的文本片段没有左邻字，位于文本结尾的文本片段也没有右邻字。如果直接认为这些文本片段不存在左/右邻字就会
# 降低它的左/右熵，因此当文本片段的左/右邻字不存在时，把左/右邻字也等同为一个空格并进行统计，
# 这样也可以修正提高位于文本开头或结尾的文本片段的左/右熵。


# 3.对文本片段进行多次筛选，并综合考虑文本片段的词频，左/右熵，聚合度等信息。
# 首先，过滤掉字长为1的文本片段和中间有空白符、纯英文等不符合要求的文本片段；
# 其次，按照预设的最小词频，最小熵，最小聚合度对属性值过低的文本片段进行过滤；
# 最后，利用现有词表对发现的新词中重复的新词进行过滤。
# 文本片段的最终分数设定等于 (左熵+右熵)*聚合度*词频，对新词结果按分数进行排序，分数越高的文本片段成词的概率越高。

# --------------------------------------------------------------------------------------------------

# 优化后的运行情况：
# 在只统计二元文本片段的左右邻字集合的情况下
# 消耗时间情况：
# 对一份17M的语料文件测试，平均运行时间由原来的145s降低到103s
# 对一份153M的语料文件测试，平均运行时间由原来的1087s降低到了777s
# 将代码经过cython进一步优化后：
# 对一份17M的语料文件测试，平均运行时间由原来的145s降低到60s
# 对一份153M的语料文件测试，平均运行时间由原来的1087s降低到了420s
# 发现新词结果情况：
# 发现的新词数量比之前提高了大约一倍，整体错误率也有所下降

# --------------------------------------------------------------------------------------------------

# 特别注意：关于利用左/右邻字集合来计算左/右熵
# 将标点符号和句首句尾替换为空格后统计入邻字集合中，一般来讲空格出现次数越多，文本片段成词概率越高
# 但如果把空格也按熵的计算公式进行计算，结果会是空格出现次数越多，得到的熵越小
# 因此要把空格和普通字符分开计算，其中空格为出现次数除以(空格次数+普通字符次数)，普通字符按照熵公式计算，二者相加为左/右熵
# 例如，假设文本片段的左邻字集合为{'a':1, 'b':1, 'c':2, ' ':4}
# 之前错误的计算方法为：
# 左熵 = -1/8*ln(1/8)-1/8*ln(1/8)-1/4*ln(1/4)-1/2*ln(1/2) = 1.2130
# 正确的计算方法：
# 空格熵 = 4/8 = 0.5
# 字符熵 = -1/4*ln(1/4)-1/4*ln(1/4)-1/2*ln(1/2) = 1.0397
# 左熵 = 空格熵 + 字符熵 = 1.5397
# 可以看到经过修正后的算法，左熵相比之前有了一定的提高

# -------------------------------------------------------------------------------------------------

# 关于停用词的概念：
# 停用词是存在于文本中没有实际意义的字或词，在进行自然语言处理时需要过滤掉停用词来提高准确率和搜索效率
# 在文本处理过程中如果遇到停用词，就将停用词去掉后再继续进行处理，或者在处理前就将所有停用词先去掉
# 停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词表
# 停用词主要包括英文字符、数字、数学字符、标点符号及使用频率特高的单汉字等
# 注意：停用词表需要根据具体环境构造，没有一个停用词表可以适用于所有的环境
# 关于停用字的概念：
# 停用字都是长度为1的单字，分为左停用字表和右停用字表，常见的停用字如：的、了、是、和、不....
# 对于新词发现到的文本片段，如果文本片段的开头或结尾是停用字，就认为这个文本片段是badcase
# 上面的处理方法会有误伤，造成召回率提高，但准确率也能提高，总的效果是有提升的
# 停用字的抽取理论：
# 在大规模语料中如果某个单字的左/右邻字集合丰富程度(邻字的种类)大于某个限定值n ，就认为这个单字是左停用字/右停用字

# -------------------------------------------------------------------------------------------------

# 关于在小规模语料中发现新词：
# 一般来说，新词发现需要在大规模的语料中进行抽取，语料规模越大结果准确率越高
# 但有些时候语料的规模会很小，例如从微博上抽取新词时每条微博的都比较短或在官网上展示时用户自己输入的文本都会很短
# 在小规模语料中利用上述的方法直接抽取会产生较大的偶然性误差，结果中有较多的badcase