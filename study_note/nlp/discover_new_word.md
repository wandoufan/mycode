# 新词发现

## 参考资料
> https://blog.csdn.net/smartcat2010/article/details/77883735
> https://github.com/Moonshile/ChineseWordSegmentation
> https://github.com/yanghanxy/New-Word-Detection

## 功能简介
新词发现实际是一种不依赖任何现有词库的分词算法，可以通过分析文本片段的各种属性来判断文本片段成词的可能性。  
首先从文本中找出所有可能成词的文本片段，然后和现有的词典进行比对，词典中没有收录的即为发现的新词  

## 应用场景
在nlp中，对中文文本处理的第一步是分词，分词的重要基础就是词典  
现有的词典存在的一个重大缺陷就是词普遍老旧，未能及时收录新出现的词语，导致了对一些新词进行切分时效果不佳  
新词的出现很大程度上影响自动分词工具的准确性，研究显示,60% 的分词错误是由新词导致的  
中文环境下每天都有大量的新词语出现，例如网络新词、机构名、品牌名、人名、缩略词等等，这些新词的产生机制完全无规律可寻，例如‘抖音’，在抖音app出现之前这不是一个词，但现在就属于一个高频词，需要分词系统能够识别出来  
因此一套自动发现并收录新词的系统就对分词工作非常重要  

## 常用方法
1. 基于规则的方法利用构词原理,结合语义、词性等信息构造模板,通过匹配来发现新词  
基于规则的方法准确率高，但需要根据具体的领域来制定不同的规则，维护规则较困难，可扩展性也比较差  
2. 基于统计的方法通过对语料中的词频，左右熵，聚合度等文本片段的属性信息进行统计来发现新词  
基于统计的方法灵活度比较高，目前实际采用这种方法进行新词发现  
根据统计方法的特性，一般文本规模越大，发现的候选词准确率越高，更适合于从大规模语料中获取新词  
这种方法的查全率较高，可以通过大规模语料发现大量的的新词，但查准率一般，不一定能够发现某个特定新词  

## 基础概念

### 凝合程度/聚合程度
表示文本片段内部的字结合紧密程度，通过凝合程度来找到词的边界。  
例如,电影院的凝合程度计算为： 
``` 
aggregation('电影院') = min[p('电影院')/(p('电')*p('影院')),p('电影院')/(p('电影')*p('院'))]  
```
凝合程度高表示文本片段内部的字之间经常是固定搭配,例如：'蝙蝠'、'彷徨'、'玫瑰'、'忐忑'、'乒乓'等等。  
凝合程度越高说明文本片段能成词的概率越大。  

### 左熵/右熵
表示文本片段外部左/右搭配字的丰富程度,即自由运用程度。
通过左右熵来判断文本片段成词的概率，文本片段能够算作词,则片段应该有丰富的左邻字集合和右邻字集合,即自由运用程度高。
特别地，由于词本身的特性，可能会存在左熵很大右熵很小或右熵很大左熵很小的情况，例如，'交响'，'后遗'，'鹅卵'等词。
自由运用程度：定义为文本片段的左熵和右熵中较小的值  
例如,str='吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮','葡萄'左邻字为{吃,吐,吃,吐},右邻字为{不,皮,倒,皮}  
'葡萄'的左右熵为计算：
```
left = -(1/2)*ln(1/2)-(1/2)*ln(1/2) ≈ 0.693
right = -(1/2)*ln(1/2)- 1/4)*ln(1/4)-(1/4)*ln(1/4) ≈ 1.04
```

在实际判断中，文本片段的凝合程度和自由程度都要考虑：  
只看凝固程度的话，程序会找出'巧克'、'俄罗'、'颜六色'、'柴可夫'等实际上是'半个词'的片段；  
只看自由程度的话，程序则会把'吃了一顿'、'看了一遍'、'睡了一晚'、'去了一趟'中的'了一'提取出来;  
另外会存在一些特殊词，如'利亚'、'斯坦'等凝合度和自由度都很高，但实际并不能成词；  

## 处理流程
1. 文本语料预处理  
1.1 语料清洗  
由于文本语料的来源广泛，文本语料的格式规范也有很大的不确定性，因此对于用词发现的文本语料要预先进行数据清洗。  
首先去掉文本中的换行符等各种空白符，将文本中每一句话都拼接起来。  
然后将文本中的标点符号替换为空格，关于标点符号可以根据新词发现任务的需求来保留相应的符号。  
例如，在进行军事武器词的新词发现时，目标新词中经常会出现‘歼-20’中的‘-’，因此‘-’就需要保留下来。  
1.2 语料切割  
根据目标新词的最大词长来对语料进行切割，最大词长与具体的新词发现任务有关。  
根据统计方法的特性，词长越短，统计结果的准确度越高。  
一般我们认为中文中至少两个字才能成词，以最大词长等于4为例，我们对文本从头到尾分别切割出所有可能的长度为1、2、3、4的文本片段作为候选词。 
备注：长度为1的单字本身不能成词，但是在计算长词的聚合度时会需要用到单字词频信息  

2. 成词标准  
对于每一个候选词，可能是一个完整的词，也可能是一个不成词的文本片段。  
通过判断候选词的信息熵可以计算出候选词成词的概率。  
2.1 聚合度  
聚合度又称为内部凝固度或凝合程度，用来表示文本片段内部的字之间的结合紧密程度。  
聚合度 = 文本片段词频 / 文本片段子部分词频乘积。  
聚合度高表示文本片段内部的字之间经常是固定搭配，而不是由于随机排列被组合在了一起，例如：'蝙蝠'、'忐忑'、'乒乓'等。  
一般我们认为词语内部的字都会经常性组合在一起，即聚合度越高的候选词成词的概率越大。  
以候选词‘abc’为例，其聚合度的计算公式为： 
``` 
Aggregation =min⁡{P(abc)/P(ab)P(c) ,(P(abc))/(P(a)P(bc))}  
```
2.2 左右熵  
左右熵表示文本片段外部左/右搭配字的丰富程度,即自由运用程度。  
一般我们认为一个文本片段能够成词则它的左右两边就能搭配很多的文字，即有丰富的左邻字集合和右邻字集合。   
例如，语料为'吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮'，则其中候选词’葡萄’的左邻字集合为{吃,吐,吃,吐}，右邻字集合为{不,皮,倒,皮}，’葡萄’的左右熵计算公式为：  
```
left_entropy = -(1/2)*ln(1/2)-(1/2)*ln(1/2) ≈ 0.693  
right_entropy = -(1/2)*ln(1/2)-(1/4)*ln(1/4)-(1/4)*ln(1/4) ≈ 1.04  
```
2.3 词频  
基于大规模语料统计时，一般认为文本片段出现的频率越高，成词的概率越高  

3. 候选词进一步过滤筛选  
3.1 阈值及综合分数  
针对候选词可以分别设定聚合度、左右熵、出现词频的最小值，将不符合阈值的候选词都过滤掉，可以减少文本中由于随机性产生的不成词的文本片段。  
结合候选词的各项属性值计算出一个综合分数，计算公式为：
```
Score = (left_entropy+right_entropy)*aggregation*frequence  
```
将候选词按照综合分数由高到低排序，去掉低分段的候选词，只保留高分段的候选词。  
3.2 左右停用字  
在从语料切割出文本片段中经常会存在一些无意义的助词、语气词、量词等，如’的’,’了’,’是’等。  
可以从一个大规模的语料中统计出文本片段左右两侧最常出现的字，经过人工筛选后作为左右停用字表。  
如果候选词的左右两侧字出现在停用字表中，则认为该候选词是一个无意义的不成词的文本片段，可以将其过滤掉。  
3.3 特殊的badcase  
在中文环境下，存在一类文本片段，它的内部聚合度很高，左右两侧也有丰富的邻字集合，出现频次也很高，但它并不是一个完整的词，而是一类特殊的badcase。  
如’斯坦’本身不是一个词，但经常被包含在’巴基斯坦’，’斯坦福’,’哈萨克斯坦’中，各项指标分数都很高，会通过层层过滤出现在最终结果中。  
因此对于这一类特殊badcase需要专门再进一步过滤，暂时没有好的解决办法，可以通过人工筛选后加入停用词表来过滤  
常见的特殊badcase包括：[斯坦，利亚，尼亚，拉斯，年x月，月x日，国人民，国空军，型坦克，号线，亿美元, 轮融资，族自治县，句话，本书]  
3.4 中间停用字过滤  
如果文本片段中包含['的', '和', '与', '是', '到', '了', '为']，一般也认为不成词  
3.5 左邻字两次过滤  
新词发现中常有一类半个词的badcase，如'亿美元'、'本书'、'句话'，这类词的左右熵和聚合度都高，很难通过分数阈值将其过滤；  
但这一类词的左侧通常都与数量词或代词搭配，我们可以利用这一特性在计算阶段通过判断候选词的左邻字集合来对badcase进行过滤；  
如左邻字集合中经常出现数字['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '一' , '二' ,'三' ,'四' ,'五' ,'六' ,'七' ,'八' ,'九' ,'十' ,'零','百' ,'千' ,'万' ,'亿']或[这','那','哪','每','某','各','几', '两', '多', '上']，我们就可以认为它是badcase;  
3.6 利用现有词表过滤  
经过以上过滤后的候选词，就认为是可以成词的文本片段。将候选词和现有的词表进行比对，没有出现在现有词表中的候选词就是我们要找的目标新词。 
3.7 人工标注  
由于中文语言的复杂性，经过算法层层过滤得到的结果中还可能混杂有不成词的文本片段。  
在工业级的应用中还是需要经过人工标注进行最后的确认。  

## 算法优化实现细节
1. 通过只计算出n元文本片段的左右熵来近似替代n元以上文本片段的左右熵，其中n的取值范围为(1,最大词长) 
按照长度切分所有可能的片段后会产生大量的片段，如果对所有文本片段都计算其左右熵会耗费大量的时间，
例如，对于一个简单文本'周杰伦的歌'，在限定最大词长为4的情况下传统方法需要统计出['周', '周杰', '周杰伦', '周杰伦的','杰', '杰伦', '杰伦的', '杰伦的歌', '伦', '伦的', '伦的歌', '的', '的歌', '歌']共14个文本片段的左/右邻字集合，并分别计算其左/右熵和聚合度。
在这些文本片段中我们把所有n元及以上的文本片段的左/右邻字集合都视作等同于有相同开头/结尾的n元文本片段的左/右邻字集合；  
同样的，所有n元及以上的文本片段的左/右熵也都等同于有相同开头/结尾的n元文本片段的左/右熵
只统计出'周'的左邻字集合，然后计算出'周'的左熵，'周杰伦'的左熵就可以等同于'周'的左熵；  
但实际上这样操作也会造成一定误差，在同一份语料中，'周杰伦'的左邻字集合一定'周'的左邻字集合，但'周'的左邻字集合不一定是'周杰伦'的左邻字集合；
例如，在一份测试语料中'周'有左邻字'上'来组成'上周'，但'周杰伦'并没有存在左邻字'上'，这样操作会造成'周杰伦'的左熵偏高
这个问题在只统计一元文本片段时误差较为明显，当只统计二元或二元以上的文本片段时误差就会大大降低。在同一份语料中只统计n元文本片段时，
当n取值越大，误差就越小。一般我们认为两个字就可以成词，因此在实际操作中，当n取值3或以上时还会存在另一个问题：例如，在同一份语料中，
我们只统计三元文本片段的左/右邻字集合，但我们不能直接把'周杰伦'的左邻字集合等同于'周杰'的左邻字集合，因为语料中可能还存在'周杰杰'
等其他以'周杰'开头的三元文本片段。
经过多次测试发现，在限定最大词长为4的情况下只统计二元文本片段的左/右邻字集合可以较好的兼顾性能和效果，
不仅可以大幅降低程序的运行时间，同时也能保证发现新词结果的准确率。

一篇提高左右熵计算速度的论文，待研究：  
> https://wenku.baidu.com/view/2738b812cc7931b765ce1541.html

2. 将左右邻字是标点符号等非中文字符的情况都视作邻字为空格  
将空格也计入左右邻字集合中，并对没有左右邻字的情况补充空格  
一般位于开头/结尾位置的词没有左右邻字，这种情况下视作左右邻字为空格  

3. 计算熵时分别计算空格和非空格字符  
注意：不能把邻字集合中的空格和非空格等同起来计算熵  
一般来讲空格出现次数越多，文本片段成词概率越高；但如果把二者等同起来之后，空格越多，计算得到的熵反而越小  
假设某个候选词左邻字集合为{'a', 'b', ' ', ' '}，即集合中含有2个普通字符和2个空格  
错误的计算方法为：
```
空格为2个的情况下
entropy = -(1/4)*ln(1/4) -(1/4)*ln(1/4) -(1/2)*ln(1/2) = 0.693 + 0.346 = 1.04
假设左邻字集合的空格由2个增加到6个
entropy = -(1/8)*ln(1/8) -(1/8)*ln(1/8) -(3/4)*ln(3/4) = 0.259 + 0.215 = 0.474
```
两种正确的计算方法：
a. 空格按照概率公式单独计算
```
entropy = -(1/4)*ln(1/4) -(1/4)*ln(1/4) + 2/4
```  
b. 空格合并入熵计算公式 
```
entropy = -(1/4)*ln(1/4) -(1/4)*ln(1/4) - 2*(1/4)*ln(1/4)
```
注意： 按照b方法计算时每个空格都单独算作一种字符，而不是所有空格加起来算作一种字符  


## 左右停用词参考：
注意：停用词表需要根据具体环境构造，没有一个停用词表可以适用于所有的环境 
```
    'left_stop_word': ['的', '一', '和', '之', '等', '在', '不', '是', '也', '为', '与', '有', \
    '就', '了', '以', '这', '时', '1', '出', '而', '还', '都', '来', '可', '对', '后', '到', '自', \
    '成', '或', '2', '作', '发', '被', '最', '进', '所', '开', '下', '生', '无', '几',\
    '相', '3', '4', '5', '6', '7', '8', '9', '0', '从', '怎', '又', '及', '更', '很', '已', '跟', '让',\
    '两', '要', '给', '那', '哪', '该', '每', '没', '能', '什', '如', '会', '们', '你', '我', '他', '它', '她',\
    '些', '型', '但', '既', '此', '由', '说', '号', '把', '走', '叫', '去', '找'],
    'right_stop_word': ['的', '是', '和', '在', '为', '有', '了', '于', '以', '到', '与', '个', '上', \
    '用', '中', '而', '着', '对', '过', '及', '一', '将', '如', '下', '.', '出', '来', \
    '得', '成', '后', '时', '或', '种', '从', '生', '小', '多', '入', '些', '被', '地', '把', \
    '之', '其', '啦', '吧', '呢', '啊', '呀', '也', '就', '第', '等',\
    '要', '让', '们', '么', '吗', '说', '给', '都', '又', '你', '我', '他', '它', '她', '很', '不', '里']
```