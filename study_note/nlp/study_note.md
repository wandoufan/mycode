
机器学习中相关概念：

* 机器学习是一种重在寻找数据中的模式并使用这些模式来做出预测的研究和算法的门类。
机器通过分析大量数据来进行学习。比如说，不需要通过编程来识别猫或人脸，它们可以通过使用图片来进行训练，从而归纳和识别特定的目标。

训练集，验证集，测试集的作用：
在训练有监督的机器学习模型的时候，会将一个大的数据集划分为训练集、验证集和测试集，划分比例一般设置为8:1:1，划分方式为均匀随机抽样。
对原始数据进行三个集合的划分，是为了能够选出效果（可以理解为准确率）最好的、泛化能力最佳的模型。
将数据分为三个部分也是为了防止产出的模型过拟合，如果用所有的数据去训练模型，得到的结果会过拟合与原始数据，而对新数据进行训练时效果可能更差。
需要注意的是，通常都会给定训练集和测试集，而不会给验证集。这时候验证集一般都是从训练集中均匀随机抽样一部分样本作为验证集。

* 原始数据集：已经经过正确处理的大的数据集合

* 训练集(Training set)：算法从训练集中挖掘出规则，组合成分类器，并产出训练模型
即通过训练集来确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数。

* 验证集(Validation set)：用来做模型选择，即模型的最终优化和确定
验证集并不参与学习参数的确定，验证集只是为了选择超参数

* 测试集(Test set)：测试集只使用一次，即在训练完成后评价最终的模型时使用。它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型效果的评价
其中训练集和验证集均是同一对象的数据，但是测试，我们就需要用跨对象的数据来验证模型的稳定性。
值得注意的是，千万不能在训练过程中使用测试集，而后再用相同的测试集去测试模型。这样做其实是一个cheat，会造成模型过拟合。

* 超参数不能或者难以通过机器学习算法学习得出，一般由专家经过经验或者实验选定，如广义线性回归中的多项式次数，控制权值衰减的λ等

* 训练轮数：
???

* 学习方法：使用样例（或称样本，训练集）来合成计算机程序的过程称为学习方法。

* 监督学习：学习过程中使用的样例是由输入/输出对给出时，称为监督学习。最典型的监督学习例子就是文本分类问题，训练集是一些已经明确分好了类别文档组成，文档就是输入，对应的类别就是输出。

* 非监督学习：学习过程中使用的样例不包含输入/输出对，学习的任务是理解数据产生的过程。典型的非监督学习例子是聚类，类别的数量，名称，事先全都没有确定，由计算机自己观察样例来总结得出。

* 假设：计算机对训练集背后的真实模型（真实的分类规则）的猜测称为假设。

* 泛化性：一个假设能够正确分类训练集之外的新的未知数据的能力称为该假设的泛化性。

* 一致假设：一个假设能够对所有训练数据正确分类，则称这个假设是一致的。

* 过拟合：为了使匹配结果更精准而把规则设置的过于严格称为过拟合，过于严格的规则匹配样本数据会非常精准，但对于新数据匹配结果可能反而更差。

* 正样本和负样本：对某个类别来说，属于这个类别的样本文档称为正样本；不属于这个类别的文档称为负样本。

* 超平面：n维空间中的线性函数唯一确定了一个超平面。一些较直观的例子，在二维空间中，一条直线就是一个超平面；在三维空间中，一个平面就是一个超平面。

* 线性可分和不可分：如果存在一个超平面能够正确分类训练数据，并且这个程序保证收敛，这种情况称为线形可分。如果这样的超平面不存在，则称数据是线性不可分的。

-------------------------------------------------------------------------

对机器学习模型进行性能度量中的相关概念：
1.在回归任务中：
最常用的性能度量是均方误差，即多次样本的结果和标准结果进行差值比对
差值越小代表越接近于标准结果
2.在分类任务中(包括二分类任务和多分类任务)：
错误率：分类错误的样本数占样本总数的比例
精度：分类正确的样本数占样本总数的比例
即错误率+精度=1

以判断是否是恶心肿瘤为例
查准率(precision):判断是恶性肿瘤数/(判断是恶性肿瘤数+判断是良性肿瘤数)，描述准确性
即结果中是否每一个都是恶性肿瘤
查全率(recall)/召回率：判断是恶性肿瘤数/样本中所有恶性肿瘤个数，描述全面性
即样本中所有的恶性肿瘤是否都被找到
查准率和查全率是一组矛盾的参数；想要增加准确率，就需要把最具把握的样本才判定
为恶性肿瘤；想要增加全面率，就需要把所有具有可能性的样本都判定为恶性肿瘤

F1参数用来平衡查准率和查全率，在不同的应用场景下来提供查准率或查全率
F1 = 2*p*r/(p+r)

关于宏平均和微平均：
例如在多组数据中，每组数据都有其对应的查准率，查全率和F值
最后可以综合计算一个平均的F值来反映总体水平
微平均F值 = 每组F值之和/组数
宏平均F值 = 2*平均查准率*平均查全率/(平均查准率+平均查全率)

微平均和宏平均的区别，意义？？？

https://blog.csdn.net/ybdesire/article/details/53613628
https://blog.csdn.net/liweibin1994/article/details/76944056
https://www.cnblogs.com/zle1992/p/6689136.html

-------------------------------------------------------------------------

中文与英文的区别（为什么首先要做分词）：
对于我们每天打交道的中文来说，并没有类似英文空格的边界标志。
而理解句子所包含的词语，则是理解汉语语句的第一步。汉语自动分词的任务
，通俗地说，就是要由机器在文本中的词与词之间自动加上空格。
目前常用的分词方法有两类：
机械式分词法：以分词词典为依据，对文本中的字符串和词表中的词逐一匹配进行分词。
理解式分词法：利用汉语的语法知识，语义知识和心理学知识，需要建立分词数据库，
知识库和推理库，属于理想的方法，但目前还不成熟。

--------------------------------------------------------------------------

文本分类相关：
https://blog.csdn.net/banrixianxin/article/details/25928967
https://www.douban.com/note/627665166/?type=like

* 自动文本分类(Automatic Text Categorization)简称文本分类，是指计算机将一篇文章归于预先给定的某一类或某几类的过程。
文本分类会按照预先定义的主题类别，为文档合集中的每个文档确定一个类别。
文本分类问题与其它分类问题没有本质上的区别，其方法可以归结为根据待分类数据的某些特征来进行匹配，当然完全的匹配是不太可能的，因此必须（根据某种评价标准）选择最优的匹配结果，从而完成分类。

* 需要注意的两个点：
1.用于分类的类别体系是需要提前建立的，而且一旦确定就不能轻易改变，
如果要改变就相当于重建一个分类系统
2.一个文本没有严格规定只能被分配给一个类别，分在不同类别的置信度可能不一样

* 文本分类的应用场景：
对文本集按照主题进行分类，判断文章写作风格，判断作者态度，判断是否是机器人
写的文章，判断是否是垃圾邮件，等等.....

* 文本分类和网页分类的区别：
文本分类类似于网页分类，但网页分类可以借助文件的编码格式，文章作者，发布日期
，网页链接，网页结构等信息进行分类判断，而文本分类只能根据文章的文字内容进行分类

* 文本分类的几种方法：
1.词法匹配，最初版的方法通过判断文本中是否出现指定关键词来判断文本的类别，但实际的效果不好。
2.知识工程，改进版的方法通过人工来提前对文本进行分析，为文本建立起推理匹配规则，
但这种方法的效果严重依赖于匹配规则的质量，而且一套规则只能针对一类文本，成本较高且不具有灵活性。
3.目前常用是统计学习方法(机器学习方法)，需要提前准备一批由人工进行了精准分类
的文档作为学习材料(即训练集/样本/样本数据)，计算机自动从文档中挖掘出来有效的分类规则
(过程称为训练/training)，总结出来的规则合集称为分类器，训练完成后就可以对新文档使用分类器进行分类处理。
常用的算法有支持向量机SVM,朴素贝叶斯,KNN等
4.深度学习方法是对统计学习方法的改进，统计机器学习方法首先需要进行特征工程工作，该工作需要深入理解业务需求，并且非常耗时耗力。
而深度学习方法可以自动的提取特征，使人们将注意力集中在数据和模型上。
常用的算法有CNN模型,fastText模型等。

* 一般把学习模型在实际使用中遇到的数据称为测试数据；为了加以区分，模型评估
与选择中用于评估测试的数据集称为验证集

* 统计学习方法的思想：
一般认为文档的内容与其中所包含的词有着必然的联系，同一类文档之间总存在多个共同的词，而不同类的文档所包含的词之间差异很大；另外词出现的次数对分类也很重要。
空间向量模型(VSM)把样本数据转化为向量表示，通过记录关键词和词的权重来实现对文本的解析，实现简单，准确度也较高。词的权重不一定就是词频，更常用的是TF-IDF参数。
文本作为信息的载体包含的信息包括：词元素本身的信息，元素之间组成顺序带来的信息，以及上下文之间的信息。而VSM模型只用到了词本身信息，忽略了其他方面的信息。
潜在语义索引(LSI)模型相比VSM模型可以保留更多的语义信息，但目前还不成熟。

* 统计学习方法的实现过程：
1.把文本转化成向量表示
存在的问题是包含所有中文词语的词典向量太大，用来存储一篇只有几千词的文本会浪费空间。
首先会进行一个'去停止词/停用词'的预处理过程，即根据停用词词典，去掉文本带有一些通用的,无法用来判断文本类别的词语，如'我们'，'事情'，'里面'等词。
注意：'去停用词'的场景比较模糊，不经常会进行该步骤
另外会通过'特征提取'/'降维'/'TSR'(Term Space Reduction)特征空间的压缩过程来选取出最具代表性的词语，来进一步减少词典向量。
特征提取有两类方法：
特征选择：从原有特征中选出少量更具有代表性的特征，特征的数量减少但类型并未变化。
特征抽取：从原有特征中重构出新的特征，如LSI模型转化为矩阵，文档生成模型转化为概率分布参数。新特征更具代表性，且耗费资源更少。
2.构造分类器
对同一个文本数据，同一个向量模型，关注不同的特性不同的指标就会得到不同的结论。
常见的分类算法有决策树，Rocchio，朴素贝叶斯(Naive Bayes)，神经网络，支持向量机，线性最小平方拟合，kNN，遗传算法，最大熵，Generalized Instance Set等等，但这些算法都有各种缺陷。
目前性能最好的算法是支持向量机(Support Vector Machine)/SVM算法，SVM分类器的优点在于通用性较好，且分类精度高,分类速度快,分类速度与训练样本个数无关，在查准和查全率方面都优于kNN及朴素贝叶斯方法。

-----------------------------------------------------------------------------------------

新词发现相关：
https://blog.csdn.net/smartcat2010/article/details/77883735

* 注意：新词发现其实也是一种特殊的分词方法，它的分词算法不需要语料库或知识库就可以把新的词语从文本中抽取出来。抽取到的词和原有的词库进行比对，没有的即可视为新词。

* 但在中文分词领域里，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？更惨的则是机构名、品牌名、专业名词、缩略语、网络新词等等，它们的产生机制似乎完全无规律可寻。最近十年来，中文分词领域都在集中攻克这一难关。自动发现新词成为了关键的环节。

* 怎样的文本片段才算一个词？大家想到的第一个标准或许是，看这个文本片段出现的次数是否足够多。我们可以把所有出现频数超过某个阈值的片段提取出来，作为该语料中的词汇输出。不过，光是出现频数高还不够，一个经常出现的文本片段有可能不是一个词，而是多个词构成的词组。

* 为了算出一个文本片段的凝合程度，我们需要枚举它的凝合方式——这个文本片段是由哪两部分组合而来的。令 p(x) 为文本片段 x 在整个语料中出现的概率，那么我们定义“电影院”的凝合程度就是 p(电影院) 与 p(电) · p(影院) 比值和 p(电影院) 与 p(电影) · p(院) 的比值中的较小值，“的电影”的凝合程度则是 p(的电影) 分别除以 p(的) · p(电影) 和 p(的电) · p(影) 所得的商的较小值。
为了证明“电影院”一词的内部凝固程度确实很高，我们可以计算一下，如果“电影”和“院”真的是各自独立地在文本中随机出现，它俩正好拼到一起的概率会有多小。在整个 2400 万字的数据中，“电影”一共出现了 2774 次，出现的概率约为 0.000113 。“院”字则出现了 4797 次，出现的概率约为 0.0001969 。如果两者之间真的毫无关系，它们恰好拼在了一起的概率就应该是 0.000113 × 0.0001969 ，约为 2.223 × 10-8次方。但事实上，“电影院”在语料中一共出现了 175 次，出现概率约为 7.183 × 10-6
次方，是预测值的 300 多倍。类似地，统计可得“的”字的出现概率约为 0.0166 ，因而“的”和“电影”随机组合到了一起的理论概率值为 0.0166 × 0.000113 ，约为 1.875 × 10-6，这与“的电影”出现的真实概率很接近——真实概率约为 1.6 × 10-5
次方，是预测值的 8.5 倍。计算结果表明，“电影院”更可能是一个有意义的搭配，而“的电影”则更像是“的”和“电影”这两个成分偶然拼到一起的。

* 文本片段的自由运用程度也是判断它是否成词的重要标准。如果一个文本片段能够算作一个词的话，它应该能够灵活地出现在各种不同的环境中，具有非常丰富的左邻字集合和右邻字集合。
把一个文本片段的自由运用程度定义为它的左邻字信息熵和右邻字信息熵中的较小值
“信息熵”是一个非常神奇的概念，它能够反映知道一个事件的结果后平均会给你带来多大的信息量。如果某个结果的发生概率为 p ，当你知道它确实发生了，你得到的信息量就被定义为 - log(p) 。 p 越小，你得到的信息量就越大。如果一颗骰子的六个面分别是 1 、 1 、 1 、 2 、 2 、 3 ，那么你知道了投掷的结果是 1 时可能并不会那么吃惊，它给你带来的信息量是 - log(1/2) ，约为 0.693 。知道投掷结果是 2 ，给你带来的信息量则是 - log(1/3) ≈ 1.0986 。知道投掷结果是 3 ，给你带来的信息量则有 - log(1/6) ≈ 1.79 。但是，你只有 1/2 的机会得到 0.693 的信息量，只有 1/3 的机会得到 1.0986 的信息量，只有 1/6 的机会得到 1.79 的信息量，因而平均情况下你会得到 0.693/2 + 1.0986/3 + 1.79/6 ≈ 1.0114 的信息量。这个 1.0114 就是那颗骰子的信息熵。现在，假如某颗骰子有 100 个面，其中 99 个面都是 1 ，只有一个面上写的 2 。知道骰子的抛掷结果是 2 会给你带来一个巨大无比的信息量，它等于 - log(1/100) ，约为 4.605 ；但你只有百分之一的概率获取到这么大的信息量，其他情况下你只能得到 - log(99/100) ≈ 0.01005 的信息量。平均情况下，你只能获得 0.056 的信息量，这就是这颗骰子的信息熵。再考虑一个最极端的情况：如果一颗骰子的六个面都是 1 ，投掷它不会给你带来任何信息，它的信息熵为 - log(1) = 0 。什么时候信息熵会更大呢？换句话说，发生了怎样的事件之后，你最想问一下它的结果如何？直觉上看，当然就是那些结果最不确定的事件。没错，信息熵直观地反映了一个事件的结果有多么的随机

* 在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出“巧克”、“俄罗”、“颜六色”、“柴可夫”等实际上是“半个词”的片段；只看自由程度的话，程序则会把“吃了一顿”、“看了一遍”、“睡了一晚”、“去了一趟”中的“了一”提取出来，因为它的左右邻字都太丰富了。

* 我们把文本中出现过的所有长度不超过 d 的子串都当作潜在的词（即候选词，其中 d 为自己设定的候选词长度上限，我设定的值为 5 ），再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。为了提高效率，我们可以把语料全文视作一整个字符串，并对该字符串的所有后缀按字典序排序。