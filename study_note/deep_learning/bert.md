# BERT（Bidirectional Encoder Representations from Transformers）

## 参考资料
> https://blog.csdn.net/hpulfc/article/details/80448570
> https://zhuanlan.zhihu.com/p/46652512


## 基本概念
BERT是来自于google的用于的用来产生词向量的相关模型，即双向Transformer的Encoder  
相比于word2vec模型，BERT进一步增加了词向量模型的泛化能力，充分描述字符级、词级、句子级甚至句间关系特征  
可以用于问答系统，情感分析，垃圾邮件过滤，命名实体识别，文档聚类等任务中，作为这些任务的基础设施即语言模型  


## BERT的优势
BERT将传统大量在下游具体NLP任务中做的操作转移到预训练词向量中，在获得使用BERT词向量后，最终只需在词向量上加简单的MLP或线性分类器即可快速执行下游nlp任务  
下游NLP任务包括：  
序列标注：分词、实体识别、语义标注...  
分类任务：文本分类、情感计算...  
句子关系判断：问答系统、自然语言推理...  
生产式任务：机器翻译、文本摘要...  


## 原理简述
1. BERT的创新点在于它预训练词向量的过程是双向的，对语境的理解会比单向的语言模型更深刻  
如果使用预训练模型处理其他任务，那需要的肯定不止某个词左边的信息，而是左右两边的信息  
之前的模型是从左向右输入一个文本序列(word2vec)，或者将left-to-right和right-to-left的分别训练然后拼接起来(ELMo)  


2. BERT 利用了 Transformer 的 encoder 部分
Transformer 是一种注意力机制，可以学习文本中单词之间的上下文关系的  
Transformer 的原型包括两个独立的机制，一个 encoder 负责接收文本作为输入，一个 decoder 负责预测任务的结果
BERT 的目标是生成语言模型，所以只需要 encoder 机制
Transformer 的 encoder 是一次性读取整个文本序列，而不是从左到右或从右到左地按顺序读取，
这个特征使得模型能够基于单词的两侧学习，相当于是一个双向的功能

## Attention注意力机制
人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息  
深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更加关键的信息  
简单来说，attention就是对输入的信息赋予不同的权重  
注意模型旨在通过允许解码器访问整个编码的输入序列（h1，h2，…，ht）来减轻这些挑战。其核心思想是在输入序列上引入注意权重α，以优先考虑存在相关信息的位置集，以生成下一个输出token  
网络结构中的注意力模块负责自动学习注意力权重αij，它可以自动捕获hi（编码器隐藏状态，我们称之为候选状态）和sj（解码器隐藏状态，我们称之为查询状态）之间的相关性  
这些注意力权重用于构建内容向量C，该向量作为输入传递给解码器  
在每个解码位置j，内容向量cj是编码器所有隐藏状态及其相应注意权的加权和  

## Encoder-Decoder框架
Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，目前大多数注意力模型附着在Encoder-Decoder框架下  
文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型，就是整个系统根据输入句子Source生成了目标句子Target
1. 如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架  
2. 如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架  
3. 如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架    

## 使用BERT
BERT 可以用于各种NLP任务，只需在核心模型中添加一个层，例如：  
1. 在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层  
2. 在问答任务中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型  
3. 在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层  
