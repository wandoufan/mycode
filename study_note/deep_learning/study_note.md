# 机器学习中的一些基本概念

## 基础概念
1. 过拟合  
为了使匹配结果更精准而把规则设置的过于严格称为过拟合  
过于严格的规则匹配样本数据会非常精准，但对于新数据匹配结果可能反而更差  
2. 正样本和负样本  
对某个类别来说，属于这个类别的样本文档称为正样本，不属于这个类别的文档称为负样本  
3. 超平面  
n维空间中的线性函数唯一确定了一个超平面  在二维空间中，一条直线就是一个超平面；在三维空间中，一个平面就是一个超平面  
4. 线性可分和线性不可分  
如果存在一个超平面能够正确分类训练数据，并且这个程序保证收敛，这种情况称为线形可分  如果这样的超平面不存在，则称数据是线性不可分的  
5. 梯度下降  
梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)  
在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值  
反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了  
梯度下降有些类似于二分法，在求解的过程中，每次的求解步长都越来越小  
6. 函数饱和性  
当函数输入值很大时，随着输入值的继续增加，输出结果几乎没有变化，会导致梯度消失，这称为函数具有饱和性  
饱和按照输入值x的取值范围分为左饱和和右饱和，但函数同时满足左饱和和右饱和时称为饱和函数  
7. 硬饱和  
随着输入值x的增加输出值y恒定在一个值上称为硬饱和，如阶跃函数  
8. 软饱和  
随着输入值x的增加输出值y缓慢变化称为软饱和，如sigmoid函数  
9. 神经元  
神经元是神经网络中的基本单位，一个线性函数就可以看作一个简单的神经元  
10. 深度学习  
当神经网络中隐藏层大于1，即总层数大于3时机器学习就可以成为深度学习  
11. 超参数  
超参数是开始学习过程之前就设置好的参数，而不是通过训练得到的参数数据  
例如神经网络的学习率、隐藏层数目、激活函数等都是超参数  
学习过程中要对超参数不断优化调制，最终得到一组最优超参数  
12. 参数
机器学习中的参数不是人为设定的，而是在训练中不断迭代优化得到的数值，并作为最终模型的一部分保存下来  
13. 全连接  
第N层与第N-1层的所有神经元连接，也叫全连接  


## 训练集、验证集和测试集
在训练有监督的机器学习模型的时候，会将一个大的原始数据集划分为训练集、验证集和测试集，划分比例一般设置为8:1:1，划分方式为均匀随机抽样  
将数据分为三个部分是为了防止产出的模型过拟合，如果用所有的数据去训练模型，得到的结果会与原始数据过拟合，而对新数据进行训练时效果可能更差  
通常都会给定训练集和测试集，而不会给验证集，这时候验证集一般都是从训练集中均匀随机抽样一部分样本作为验证集  
1. 原始数据集  
原始数据集已经经过正确处理的大的数据集合  
2. 训练集(Training set)  
算法从训练集中挖掘出规则，组合成分类器，并产出训练模型  
即通过训练集来确定模型的权重和偏置这些参数  
3. 验证集(Validation set)  
用来做模型选择，即模型的最终优化和确定  
验证集并不参与学习参数的确定，验证集只是为了选择超参数  
4. 测试集(Test set)  
测试集只使用一次，即在训练完成后评价最终的模型时使用  
它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型效果的评价  
千万不能在训练过程中使用测试集，而后再用相同的测试集去测试模型，这样做其实是一个cheat，会造成模型过拟合  


## 监督学习和非监督学习
1. 监督学习  
学习过程中使用的样例是由输入/输出对给出时，称为监督学习  典型的监督学习例子就是文本分类问题，训练集是一些已经明确分好类别文档组成，文档就是输入，对应的类别就是输出  
2. 非监督学习  
学习过程中使用的样例不包含输入/输出对，称为非监督学习  
典型的非监督学习例子是聚类，类别的数量，名称，事先全都没有确定，由计算机自己观察样例来总结得出  


## 评估机器学习模型的性能
1. 在回归任务中  
最常用的性能度量是均方误差，即多次样本的结果和标准结果进行差值比对，差值越小代表越接近于标准结果  
2. 在分类任务中(包括二分类任务和多分类任务)  
错误率：分类错误的样本数占样本总数的比例  
精度：分类正确的样本数占样本总数的比例  
即错误率+精度=1  


## 查准率、查全率和F值
以判断是否是恶性肿瘤为例  
1. 查准率(precision)  
判断是恶性肿瘤数/(判断是恶性肿瘤数+判断是良性肿瘤数)，描述准确性，即结果中是否每一个都是恶性肿瘤  
2. 查全率(recall)/召回率  
判断是恶性肿瘤数/样本中所有恶性肿瘤个数，描述全面性，即样本中所有的恶性肿瘤是否都被找到  
3. 查准率和查全率的关系  
查准率和查全率是一组矛盾的参数  
想要增加准确率，就需要把最具把握的样本才判定为恶性肿瘤  想要增加全面率，就需要把所有具有可能性的样本都判定为恶性肿瘤  
4. F值  
F1参数用来平衡查准率和查全率，F1 = 2*p*r/(p+r)  
例如在多组数据中，每组数据都有其对应的查准率，查全率和F值  
最后可以综合计算一个平均的F值来反映总体水平  
5. 微平均F值  
微平均F值 = 每组F值之和/组数  
6. 宏平均F值  
宏平均F值 = 2*平均查准率*平均查全率/(平均查准率+平均查全率)  


## 神经网络的结构
神经网络是一种分层结构，一般由输入层、隐藏层、输出层组成，即神经网络至少有3层  
当隐藏层大于1，总层数大于3时就可以成为深度学习  
1. 输入层  
输入层接收原始数据，然后传送到隐藏层  
2. 隐藏层  
隐藏层对数据进行特征提取，把前一层的输出变成后一层的输入，本质上相当于对数据进行空间坐标变换  
3. 输出层  
输出层完成神经网络的决策输出  


## 神经网络的特点
1. 输入向量的特征维度和输入层的神经元个数相同，即一一对应  
2. 同一层神经元之间可能没有连接，也可能有连接，如RNN  
3. 神经网络有多种拓扑结构，不一定都是全连接，如CNN 
4. 每个连接都设置有权值  
5. 每层神经元的数量不一定相同，即输入向量和输出向量的维度数可能不同  


## 学习率
1. 学习速率是用来调整损失函数变化速度的超参数，可以简单理解为收敛速度  
2. 学习率越低，损失函数的变化速度就越慢，达到收敛所需要的迭代次数会非常高  
使用低学习率可以确保我们不会错过任何局部极小值，但也意味着我们将花费更长的时间来进行收敛  
3. 学习率越大，损失函数的变化速度就越快，但最终可能无法收敛  
使用高学习率在算法优化的前期会加速学习，使得模型更容易接近局部或全局最优解  
但是在后期会有较大波动，甚至出现损失函数的值围绕最小值徘徊，波动很大，始终难以达到最优  
4. 神经网络为了求解出损失函数的最小值一般都是梯度下降的，即隐藏层的学习率是逐层减少的  
在模型训练初期，会使用较大的学习率进行模型优化，随着迭代次数增加，学习率会逐渐进行减小，保证模型在训练后期不会有太大的波动，从而更加接近最优解  


## 梯度消失和梯度爆炸
1. 基本概念  
在正常的神经网络中，为了求解出求解损失函数的最小值，都是采用梯度下降的方法进行迭代求解  
当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了，这种现象叫做消失的梯度问题，梯度消失和梯度爆炸实际指的是同一种情况  
2. 梯度消失和梯度爆炸的产生原因  
梯度消失和梯度爆炸都属于梯度不稳定，隐藏层层数过多、激活函数选取不当等都会造成梯度不稳定  
本质上的原因是网络层数太深而引发的梯度反向传播中的连乘效应  
3. 梯度消失的解决办法  
3.1 激活函数使用relu来代替sigmoid  
3.2 LSTM的结构设计也可以改善RNN中的梯度消失问题  
4. 梯度消失带来的问题  
当梯度消失发生时，接近于输出层的隐藏层梯度相对正常，因此可以正常进行权值更新  
但靠近于输入层的隐藏层学习率太低，权值更新缓慢甚至更新停滞，导致训练时只等价于后面几层的浅层网络学习  


## 损失函数/代价函数
1. 损失函数的概念  
损失函数用来计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行  
2. 损失函数的使用步骤  
2.1 用随机值初始化前向计算公式的参数  
2.2 代入样本，计算输出的预测值  
2.3 用损失函数计算预测值和标签值（真实值）的误差  
2.4 根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值  
2.5 继续执行2.2，直到损失函数值达到设定的阈值就停止迭代  
3. 常用损失函数  
3.1 方差损失函数  
这种损失是通过计算实际(目标)值和预测值之间的平方差的平均值来计算的，适用于回归任务  
3.2 对数损失函数  
3.3 交叉熵损失函数  


## 反向传播算法BP(Back Propagation)
后向传播算法在评估误差后从后向前的进行各个权重值的调整，从而使下次迭代的误差减小，实质是参数优化的过程  
这种调整是从后向前进行的，即由输出层经过每个隐藏层，最终传递到第一个隐藏层，因此也称为后向传播算法  


## 激活函数
1. 激活函数的概念  
神经网络处理的数据往往是线性不可分的，需要有处理非线性数据的能力  
一个没有激活函数的神经网络将只不过是一个线性回归模型，解决不了非线性问题  
如果激活函数是线性的，那么不管引入多少隐层，其效果和单层感知机没有任何差别  
在所有的隐层和输出层加一个激活函数就可以得到一个非线性的神经网络模型  
2. 参考资料  
> https://blog.csdn.net/program_developer/article/details/78704224
3. 激活函数的作用  
激活函数用来将线性函数转化为非线性函数，提升神经网络模型的表达能力  
4. 激活函数的特性  
激活函数应该是一条顺滑的曲线，即处处可导  
激活函数应该是非线性的，不能是一条直线，即导数不是常数  
激活函数的输入值可以在[−∞,+∞]之间，但应该是有限的输出范围  
激活函数应该具有单调性，只能单调增长或单调减少，即导数符号只能正或负  
激活函数应该具有非饱和性，否则会造成系统收敛迭代慢  
5. 常用激活函数样例  
* sigmoid函数(是一个S形曲线)，优点是单调连续，输出范围有限(在0到1之间)，求导容易；  缺点是具有软饱和性，容易产生梯度消失，导致训练出现问题，另外输出范围也不是以0为中心的。  
```
def sigmoid(x):
    """
    Sigmoid函数是单增长的S型函数，常被用作神经网络的阈值函数，可以将变量x(x范围在负无穷到正无穷之间)映射到0,1之间
    其中math.exp(-x)代表自然常数e的-x次方， 函数图像是第一二象限间的S型的单调增长函数，其中x = 0时y = 1/2
    """
    return 1.0 / (1 + math.exp(-x))
```
* tanh函数，优点是比sigmoid函数收敛速度更快，输出范围以0为中心；缺点是同样存在饱和性产生的梯度消失问题。  
```
def tanh(x):
    """
    tanh函数即双曲正切函数，可以将变量x(x范围在负无穷到正无穷之间)映射到-1,1之间
    函数图像是第一三象限间的S型单调增长函数，其中x = 0 时y = 0
    """
    return (math.exp(x) - math.exp(-x))/(math.exp(x) + math.exp(-x))
```
* ReLU函数，优点是能够更快速的收敛，有效缓解了梯度消失的问题，在无监督的训练中也有较好的效果；缺点是训练中可能会出现神经元死亡，权重无法更新的情况。  
```
def relu(x):
    """
    relu函数即线性整流函数，又称线性修正单元，具有线性、非饱和的特性
    """
    if x <= 0:
        return 0
    else:
        return x
```