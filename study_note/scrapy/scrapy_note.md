# 主要记录爬虫方面的基本知识：
* 参考资料：
> http://www.runoob.com/w3cnote/python-spider-intro.html


## python爬虫的一般流程

### 1.发起请求
* 使用http库向目标站点发送一个request请求，需要构建一个合理的url

### 2.获取相应内容
* 如果服务器正常相应，会通过网页下载器获得一个response，response是一个网页字符串，包含网页中的html、json、图片、视频等
* 常用的库包括：
* urllib(python自带的库)
* requests(第三方库)

### 3.网页内容解析
* 通过各种工具从网页字符串中提取出我们需要的内容
* 常用的库包括：
* 正则表达式re(直观的从网页字符串中提取内容，但当网页比较复杂时使用比较困难)
* html.parser(python自带的解析工具)
* beautifulsoup(第三方库，功能强大，可以解析html和xml)
* lxml(第三方库，可以解析html和xml)
* pyquery(第三方库，用来解析网页)
* xpath(解析速度很快)

### 4.保存数据
* 将从网页中解析出的数据保存起来
* 常用的数据库包括：
* mysql
* redis


## 避免爬虫被封锁的注意点：
* 1.构建一个合理的头部header才可以伪装成合法的浏览器请求，避免被反爬机制拦截
* 2.设置一个有效的cookie，并保持cookie有效，在cookie超时时重新加载cookie
* 3.设置随机的访问时间，爬取间隔不要太频繁
* 4.设置代理ip池，使用多个ip进行数据爬取

## 常见反爬虫机制
* 1.请求头检查，比如cookies，user-agent，refer，甚至Accept-Language等等，这也是最基本的反爬机制
* 2.访问频次检查，如果一个ip在短时间内访问次服务器次数过于频繁，且cookies相同，则会被判定为机器人，你可能会被要求登录后再访问服务器或者输入验证码，甚至直接封禁你的ip
* 3.验证码验证，爬虫无法轻易绕过这一关
* 4.有些网页的元素是动态生成的，只有在js加载完成后才会显示。比如很多实用了Ajax的网站，都会有动态生成的元素。直接爬取页面将无法获取想要的元素
* 5.表单安全措施，如服务器生成的随机变量加入字段检测，提交表单蜜罐等