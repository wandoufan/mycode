# scrapy是Python最常用的爬虫框架，可以实现数据爬取，数据清洗等整个过程；
# 可以替代urllib,beautifulsoup等模块的功能？
# 参考资料：
# http://www.runoob.com/w3cnote/scrapy-detail.html

# scrapy框架包括以下组件：
# 1.Scrapy引擎(Scrapy Engine)
# Scrapy引擎是整个框架的核心，用来控制整个爬虫流程，负责数据和信号在不同模块之间的传递

# 2.调度器(Scheduler)
# 调度器负责存储管理要爬取的url，同时去除重复的url，避免爬取了重复的数据
# 调度器类似一个优先级队列，接收引擎提供的要爬取的url，输出下一个要爬取的url，队列为空时爬取结束

# 3.下载器(Downloader)
# 下载器负责下载引擎传过来的requests请求，并将响应结果返回给引擎，类似requests库的功能

# 4. 爬虫(Spider)
# 爬虫负责从网页中提取需要的信息，即实体(Item)，这部分要求用户自己定制需要提取的数据
# 另外还可以从网页中解析出其他的url，并提交给引擎，之后传入调度器中

# 5.实体管道(Item Pipeline)
# 实体管道负责处理爬虫提取出的实体数据，主要的功能是持久化实体(如存储起来)、验证实体的有效性、清除不需要的信息
# 这部分要求用户自己定制对数据的处理方法

# scrapy框架的去重原理：
# 1.调度器提供了去重功能，防止重复爬取，对接收到的每个url请求都会根据请求的相关信息加密得到一个指纹信息，
# 之后将指纹信息和set()集合中的指纹信息进行比对，如果set()集合中已经存在这个数据，就不在将这个Request放入队列中
# 2.如果set()集合中没有这个指纹信息，就将指纹加入set()中，并将这个Request对象放入队列中，等待被调度
# 3.scrapy源码中可以找到一个dupefilters.py去重器，需要将dont_filter设置为False开启去重，默认是True，没有开启去重